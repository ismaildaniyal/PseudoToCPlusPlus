{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10863156,"sourceType":"datasetVersion","datasetId":6748509}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Define file path\nuploaded_tsv_path = \"/kaggle/input/spoc-data/spoc-train.tsv\"\n\n# Check if file exists\nif os.path.exists(uploaded_tsv_path):\n    print(\"TSV file found:\", uploaded_tsv_path)\nelse:\n    raise FileNotFoundError(\"TSV file not found!\")\n\n# Load TSV file\ndf = pd.read_csv(uploaded_tsv_path, sep=\"\\t\")\n\n# Rename columns\ndf.rename(columns={\"text\": \"pseudocode\", \"code\": \"code\"}, inplace=True)\n\n# Define CSV file path\ncsv_path = \"/kaggle/working/spoc-train.csv\"\n\n# Save selected columns to CSV\ndf[[\"pseudocode\", \"code\"]].to_csv(csv_path, index=False)\n\n# Load CSV file for further operations\ndf_csv = pd.read_csv(csv_path)\n\n# Print first five rows of the CSV-loaded DataFrame\nprint(df_csv.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:45:59.061709Z","iopub.execute_input":"2025-03-13T14:45:59.062007Z","iopub.status.idle":"2025-03-13T14:45:59.991843Z","shell.execute_reply.started":"2025-03-13T14:45:59.061983Z","shell.execute_reply":"2025-03-13T14:45:59.991010Z"}},"outputs":[{"name":"stdout","text":"TSV file found: /kaggle/input/spoc-data/spoc-train.tsv\n                                        pseudocode  \\\n0                in the function gcd(a,b=integers)   \n1  if b=1 return a, else call function gcd(b, a%b)   \n2                                              NaN   \n3                                              NaN   \n4               n , nn, ans = integers with ans =0   \n\n                             code  \n0         int gcd(int a, int b) {  \n1  return !b ? a : gcd(b, a % b);  \n2                               }  \n3                    int main() {  \n4             int n, nn, ans = 0;  \n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"def clean_text(text):\n    return \"\" if pd.isna(text) else str(text)\n\ndf_csv[\"pseudocode\"] = df_csv[\"pseudocode\"].apply(clean_text)\ndf_csv[\"code\"] = df_csv[\"code\"].apply(clean_text)\nprint(df_csv.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:46:02.832706Z","iopub.execute_input":"2025-03-13T14:46:02.833133Z","iopub.status.idle":"2025-03-13T14:46:03.168190Z","shell.execute_reply.started":"2025-03-13T14:46:02.833098Z","shell.execute_reply":"2025-03-13T14:46:03.167130Z"}},"outputs":[{"name":"stdout","text":"                                        pseudocode  \\\n0                in the function gcd(a,b=integers)   \n1  if b=1 return a, else call function gcd(b, a%b)   \n2                                                    \n3                                                    \n4               n , nn, ans = integers with ans =0   \n\n                             code  \n0         int gcd(int a, int b) {  \n1  return !b ? a : gcd(b, a % b);  \n2                               }  \n3                    int main() {  \n4             int n, nn, ans = 0;  \n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming df_csv is already loaded and cleaned\n# e.g., df_csv = pd.read_csv(\"/kaggle/working/spoc-train.csv\")\n# df_csv[\"pseudocode\"] = df_csv[\"pseudocode\"].apply(lambda x: \"\" if pd.isna(x) else str(x))\n# df_csv[\"code\"] = df_csv[\"code\"].apply(lambda x: \"\" if pd.isna(x) else str(x))\n\ndef reassign_code(df_csv):\n    # Create a working copy of the DataFrame\n    df_csv = df_csv.copy()\n    \n    # Iterate through the DataFrame rows\n    i = 0\n    while i < len(df_csv):\n        # Check if current row has missing pseudocode\n        if df_csv.loc[i, \"pseudocode\"] == \"\":\n            # Skip if it's the first row (no previous row to assign to)\n            if i == 0:\n                i += 1\n                continue\n                \n            # Check for consecutive missing pseudocode\n            if i + 1 < len(df_csv) and df_csv.loc[i + 1, \"pseudocode\"] == \"\":\n                # Handle consecutive case\n                # 1. Move current row's code to previous row (append)\n                if df_csv.loc[i, \"code\"] != \"\":\n                    if df_csv.loc[i - 1, \"code\"] != \"\":\n                        df_csv.loc[i - 1, \"code\"] += \"\\n\" + df_csv.loc[i, \"code\"]\n                    else:\n                        df_csv.loc[i - 1, \"code\"] = df_csv.loc[i, \"code\"]\n                    df_csv.loc[i, \"code\"] = \"\"  # Clear moved code\n                \n                # 2. Move next row's code to the next valid pseudocode row (prepend)\n                if i + 1 < len(df_csv) and df_csv.loc[i + 1, \"code\"] != \"\":\n                    for j in range(i + 1, len(df_csv)):\n                        if df_csv.loc[j, \"pseudocode\"] != \"\":\n                            if df_csv.loc[j, \"code\"] != \"\":\n                                # Prepend the second code to the existing code\n                                df_csv.loc[j, \"code\"] = df_csv.loc[i + 1, \"code\"] + \"\\n\" + df_csv.loc[j, \"code\"]\n                            else:\n                                df_csv.loc[j, \"code\"] = df_csv.loc[i + 1, \"code\"]\n                            df_csv.loc[i + 1, \"code\"] = \"\"  # Clear moved code\n                            break\n                    i += 2  # Skip the next row since we processed it\n                else:\n                    i += 1\n            else:\n                # Single missing pseudocode: move code to previous row (append)\n                if df_csv.loc[i, \"code\"] != \"\":\n                    if df_csv.loc[i - 1, \"code\"] != \"\":\n                        df_csv.loc[i - 1, \"code\"] += \"\\n\" + df_csv.loc[i, \"code\"]\n                    else:\n                        df_csv.loc[i - 1, \"code\"] = df_csv.loc[i, \"code\"]\n                    df_csv.loc[i, \"code\"] = \"\"  # Clear moved code\n                i += 1\n        else:\n            i += 1\n    \n    return df_csv\n\n# Apply the function to df_csv\ndf_csv = reassign_code(df_csv)\n\n# Display the result\nprint(df_csv.head(10))  # Adjust to see more rows if needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:46:12.238142Z","iopub.execute_input":"2025-03-13T14:46:12.238601Z","iopub.status.idle":"2025-03-13T14:46:29.398059Z","shell.execute_reply.started":"2025-03-13T14:46:12.238565Z","shell.execute_reply":"2025-03-13T14:46:29.396972Z"}},"outputs":[{"name":"stdout","text":"                                          pseudocode  \\\n0                  in the function gcd(a,b=integers)   \n1    if b=1 return a, else call function gcd(b, a%b)   \n2                                                      \n3                                                      \n4                 n , nn, ans = integers with ans =0   \n5                                             Read n   \n6                             for i=2 to n-1 execute   \n7                                        set nn to n   \n8  while nn is not equal to 0, set ans to ans + n...   \n9                                                      \n\n                                    code  \n0                int gcd(int a, int b) {  \n1      return !b ? a : gcd(b, a % b);\\n}  \n2                                         \n3                                         \n4      int main() {\\nint n, nn, ans = 0;  \n5                              cin >> n;  \n6     for (int i = 2; i <= n - 1; ++i) {  \n7                                nn = n;  \n8  while (nn) ans += nn % i, nn /= i;\\n}  \n9                                         \n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport re\nfrom collections import Counter\nimport math\nfrom torch.nn.utils.rnn import pad_sequence\n\n\n# ---\n# ### 1. Custom Tokenizer\n# ---\nclass CustomTokenizer:\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = {}\n        self.vocab_size = 0\n        self.special_tokens = [\"<sos>\", \"<eos>\", \"<pad>\"]\n    \n    def build_vocab(self, texts):\n        \"\"\"Build vocabulary from a list of texts.\"\"\"\n        all_words = Counter()\n        for text in texts:\n            words = self.tokenize(text)\n            all_words.update(words)\n        \n        # Add special tokens first\n        for token in self.special_tokens:\n            self.word2idx[token] = self.vocab_size\n            self.idx2word[self.vocab_size] = token\n            self.vocab_size += 1\n        \n        # Add other words\n        for word, _ in all_words.most_common():\n            if word not in self.word2idx:\n                self.word2idx[word] = self.vocab_size\n                self.idx2word[self.vocab_size] = word\n                self.vocab_size += 1\n    \n    def tokenize(self, text):\n        \"\"\"Tokenize text into words and punctuation.\"\"\"\n        return re.findall(r'\\w+|[^\\w\\s]', text.lower())\n    \n    def encode(self, text):\n        \"\"\"Convert text to token IDs.\"\"\"\n        tokens = self.tokenize(text)\n        return [self.word2idx.get(token, self.word2idx[\"<pad>\"]) for token in tokens]\n    \n    def decode(self, token_ids):\n        \"\"\"Convert token IDs back to text.\"\"\"\n        tokens = [self.idx2word.get(idx, \"<unk>\") for idx in token_ids]\n        return \" \".join(tokens)\n\n# Build vocabulary from the dataset\nall_texts = df_csv[\"pseudocode\"].tolist() + df_csv[\"code\"].tolist()\ntokenizer = CustomTokenizer()\ntokenizer.build_vocab(all_texts)\n\n# Special token IDs\nSOS_TOKEN_ID = tokenizer.word2idx[\"<sos>\"]\nEOS_TOKEN_ID = tokenizer.word2idx[\"<eos>\"]\nPAD_TOKEN_ID = tokenizer.word2idx[\"<pad>\"]\n\n# Preprocess data with <sos> and <eos>\ndef preprocess_data(row):\n    pseudo = row[\"pseudocode\"]\n    code = row[\"code\"]\n    source = [SOS_TOKEN_ID] + tokenizer.encode(pseudo) + [EOS_TOKEN_ID] if pseudo else [SOS_TOKEN_ID, EOS_TOKEN_ID]\n    target = [SOS_TOKEN_ID] + tokenizer.encode(code) + [EOS_TOKEN_ID] if code else [SOS_TOKEN_ID, EOS_TOKEN_ID]\n    return {\"source\": source, \"target\": target}\n\ntrain_data = df_csv.apply(preprocess_data, axis=1).tolist()\n\n# ---\n# ### 2. Dataset and DataLoader\n# ---\nclass TranslationDataset(data.Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return torch.tensor(self.data[idx][\"source\"]), torch.tensor(self.data[idx][\"target\"])\n\ndef collate_fn(batch):\n    \"\"\"Pad sequences in a batch to the longest length.\"\"\"\n    sources, targets = zip(*batch)\n    sources_padded = pad_sequence(sources, batch_first=True, padding_value=PAD_TOKEN_ID)\n    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN_ID)\n    return sources_padded, targets_padded\n\ndataset = TranslationDataset(train_data)\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n\n# ---\n# ### 3. Transformer Model\n# ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)\n    \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1), :].to(x.device)\n\nclass Transformer(nn.Module):\n    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=num_heads,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dim_feedforward=dff,\n            batch_first=True\n        )\n        self.fc_out = nn.Linear(d_model, vocab_size)\n    \n    def forward(self, src, tgt):\n        src_emb = self.pos_encoding(self.embedding(src))\n        tgt_emb = self.pos_encoding(self.embedding(tgt))\n        src_padding_mask = (src == PAD_TOKEN_ID)\n        tgt_padding_mask = (tgt == PAD_TOKEN_ID)\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n        out = self.transformer(\n            src_emb, tgt_emb,\n            src_key_padding_mask=src_padding_mask,\n            tgt_key_padding_mask=tgt_padding_mask,\n            tgt_mask=tgt_mask\n        )\n        return self.fc_out(out)\n\n# ---\n# ### 4. Training Setup\n# ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Transformer(\n    num_layers=4,\n    d_model=256,\n    num_heads=4,\n    dff=1024,\n    vocab_size=tokenizer.vocab_size\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\noptimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n\ndef train_epoch(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    for src, tgt in dataloader:\n        src, tgt = src.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        output = model(src, tgt[:, :-1])\n        loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# Train for 10 epochs\nfor epoch in range(10):\n    loss = train_epoch(model, dataloader, optimizer, criterion)\n    print(f\"Epoch {epoch+1}/10, Loss: {loss:.4f}\")\n\n# Save the model\ntorch.save(model.state_dict(), \"transformer_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:47:11.351588Z","iopub.execute_input":"2025-03-13T14:47:11.351956Z","iopub.status.idle":"2025-03-13T16:18:42.830232Z","shell.execute_reply.started":"2025-03-13T14:47:11.351923Z","shell.execute_reply":"2025-03-13T16:18:42.829144Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 0.7686\nEpoch 2/10, Loss: 0.4402\nEpoch 3/10, Loss: 0.3818\nEpoch 4/10, Loss: 0.3648\nEpoch 5/10, Loss: 0.3533\nEpoch 6/10, Loss: 0.3478\nEpoch 7/10, Loss: 0.3456\nEpoch 8/10, Loss: 0.3448\nEpoch 9/10, Loss: 0.3452\nEpoch 10/10, Loss: 0.3457\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"def generate_code(model, pseudocode, max_len=100):\n    model.eval()\n    with torch.no_grad():\n        # Split pseudocode into individual lines\n        pseudocode_lines = pseudocode.strip().split('\\n')\n        generated_code_lines = []\n\n        for line in pseudocode_lines:\n            # Tokenize the current line\n            src_tokens = [SOS_TOKEN_ID] + tokenizer.encode(line) + [EOS_TOKEN_ID]\n            src = torch.tensor([src_tokens]).to(device)\n            tgt = torch.tensor([[SOS_TOKEN_ID]]).to(device)\n            \n            # Generate code for the current line\n            for _ in range(max_len):\n                output = model(src, tgt)\n                next_token = output[:, -1, :].argmax(dim=-1).item()\n                if next_token == EOS_TOKEN_ID:\n                    break\n                tgt = torch.cat([tgt, torch.tensor([[next_token]]).to(device)], dim=1)\n            \n            # Decode the generated tokens and add to results\n            generated_code_lines.append(tokenizer.decode(tgt[0].tolist()))\n        \n        # Join all generated lines into a single string\n        return \"\\n\".join(generated_code_lines)\n\n# Load and test the model\nmodel.load_state_dict(torch.load(\"transformer_model.pth\", map_location=device))\n\n# Example multi-line pseudocode input\ntest_pseudo = \"\"\"\n    create integers ans1, ans2 with ans1 = 0, ans2= 0\ncreate integers n, a, b, c\ncreate constant integer maxn with maxn = 105\ncreate boolean array visit with size maxn\ncreate integer vector array adj with size maxn\ncreate 2d integer array dist with size maxn by maxn\n\n\n\"\"\"\n\nprint(f\"Pseudocode:\\n{test_pseudo}\")\nprint(f\"Generated Code:\\n{generate_code(model, test_pseudo)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:58:20.897309Z","iopub.execute_input":"2025-03-13T16:58:20.897664Z","iopub.status.idle":"2025-03-13T16:58:21.433980Z","shell.execute_reply.started":"2025-03-13T16:58:20.897638Z","shell.execute_reply":"2025-03-13T16:58:21.433069Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-47-3bf12e202c5a>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"transformer_model.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Pseudocode:\n\n    create integers ans1, ans2 with ans1 = 0, ans2= 0\ncreate integers n, a, b, c\ncreate constant integer maxn with maxn = 105\ncreate boolean array visit with size maxn\ncreate integer vector array adj with size maxn\ncreate 2d integer array dist with size maxn by maxn\n\n\n\nGenerated Code:\n<sos> int ans1 = 0 , ans2 = 0 ;\n<sos> int main ( ) { int n , a , b , c ;\n<sos> } const int maxn = 105 ;\n<sos> bool visit [ maxn ] ;\n<sos> vector < int > adj [ maxn ] ;\n<sos> int dist [ maxn ] [ maxn ] ;\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import pickle\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:21:09.101857Z","iopub.execute_input":"2025-03-13T16:21:09.102167Z","iopub.status.idle":"2025-03-13T16:21:09.109372Z","shell.execute_reply.started":"2025-03-13T16:21:09.102146Z","shell.execute_reply":"2025-03-13T16:21:09.108626Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"import json\n\n# Convert tokenizer object to a dictionary (assuming it has a vocabulary attribute)\ntokenizer_dict = tokenizer.__dict__  # Extracts attributes as a dictionary\n\n# Save as JSON\nwith open(\"tokenizer.json\", \"w\") as f:\n    json.dump(tokenizer_dict, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:54:58.048070Z","iopub.execute_input":"2025-03-13T16:54:58.048425Z","iopub.status.idle":"2025-03-13T16:54:58.075471Z","shell.execute_reply.started":"2025-03-13T16:54:58.048398Z","shell.execute_reply":"2025-03-13T16:54:58.074523Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}